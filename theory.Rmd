---
title: 'Bayesian inference of conversion rates'
output: html_document
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  echo = FALSE
)
```

## Introduction

Let us explore a Bayesian approach to statistical inference in the context of
conversion rates. To begin with, let $A$ and $B$ be two random variable modeling
the conversion rates of two variants, variant A and variant Y. Variant A is
considered to be the baseline. Furthermore, let $f$ be the density function of
the joint distribution of $A$ and $B$. In what follows, concrete values assumed
by the variables are denoted by $a$ and $b$, respectively.

## Expected utility

Define the utility function as
$$
U(a, b) = G(a, b) I(a < b) + L(a, b) I(a > b)
$$
where $G$ and $L$ are referred to as the gain and loss functions, respectively.
The gain function takes effect when variant B has a higher conversion rate than
the one of variant A, and the loss function takes effect when variant A is
better than variant B, which is what is enforced by the two indicator functions.
The expected utility is then as follows:
$$
\begin{align}
E(U(A, B))
&= \int_0^1 \int_0^1 U(a, b) f(a, b) \, db \, da  \\
&= \int_0^1 \int_a^1 G(a, b) f(a, b) \, db \, da  + \int_0^1 \int_0^a L(a, b) f(a, b) \, db \, da.
\end{align}
$$

Suppose the gain and loss are linear:
$$
\begin{align}
& G(a, b) = g (b - a) \text{ and} \\
& L(a, b) = l (b - a).
\end{align}
$$
In the above, $g$ and $l$ are two positive scaling factors. Then we have that
$$
\begin{align}
E(U(A, B)) =
& g \int_0^1 \int_a^1 b f(a, b) \, db \, da  - g \int_0^1 \int_a^1 a f(a, b) \, db \, da + {} \\
& l \int_0^1 \int_0^a b f(a, b) \, db \, da  - l \int_0^1 \int_0^a a f(a, b) \, db \, da.
\end{align}
$$
For convenience, denote the four integrals by $G_1$, $G_2$, $L_1$, and $L_2$,
respectively, in which case we have that
$$
E(U(A, B)) = g \, G_1 - g \, G_2 + l \, L_1 - l \, L_2.
$$

Now, assume $A$ has a beta distribution with parameters $\alpha_a$ and
$\beta_a$; similarly, let $B$ has a beta distribution with parameters $\alpha_b$
and $\beta_b$. Assume further that, given the parameters, the variables are
independent. In this case,
$$
f(a, b) =
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b - 1} (1 - b)^{\beta_b - 1}}{B(\alpha_b, \beta_b)}.
$$

We can now compute the expected utility. The first integral is as follows:
$$
\begin{align}
G_1
&=
\int_0^1 \int_a^1
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b} (1 - b)^{\beta_b - 1}}{B(\alpha_b, \beta_b)} \, db \, da  \\
&=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
\int_0^1 \int_a^1
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b} (1 - b)^{\beta_b - 1}}{B(\alpha_b + 1, \beta_b)} \, db \, da \\
&=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_b + 1, \beta_b, \alpha_a, \beta_a)
\end{align}
$$
where
$$
h(\alpha_1, \beta_1, \alpha_2, \beta_2) = P(X_1 > X_2)
$$
for any
$$
\begin{align}
& X_1 \sim \text{Beta}(\alpha_1, \beta_1) \text{ and} \\
& X_2 \sim \text{Beta}(\alpha_2, \beta_2).
\end{align}
$$
Similarly,
$$
G_2 =
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_b, \beta_b, \alpha_a + 1, \beta_a).
$$

Regarding the last two integrals in the expression of the utility function,
$$
\begin{align}
L_1
&=
\int_0^1 \int_0^a
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b} (1 - b)^{\beta_b - 1}}{B(\alpha_b, \beta_b)} \, db \, da  \\
&=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
\int_0^1 \int_0^a
\frac{a^{\alpha_a - 1} (1 - a)^{\beta_a - 1}}{B(\alpha_a, \beta_a)}
\frac{b^{\alpha_b} (1 - b)^{\beta_b - 1}}{B(\alpha_b + 1, \beta_b)} \, db \, da \\
&=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_a, \beta_a, \alpha_b + 1, \beta_b).
\end{align}
$$
Similarly,
$$
L_2 =
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_a + 1, \beta_a, \alpha_b, \beta_b).
$$

Assembling the integrals together, we obtain
$$
\begin{align}
E(U(A, B)) =
& g \, \frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_b + 1, \beta_b, \alpha_a, \beta_a) - {} \\
& g \, \frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_b, \beta_b, \alpha_a + 1, \beta_a) + {} \\
& l \, \frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_a, \beta_a, \alpha_b + 1, \beta_b) - {} \\
& l \, \frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_a + 1, \beta_a, \alpha_b, \beta_b).
\end{align}
$$

It is worth noting that, in the case of this linear model, we have the following
relationship between $G$ and $L$:
$$
\begin{align}
G_1 - G_2
&=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
h(\alpha_b + 1, \beta_b, \alpha_a, \beta_a) -
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
h(\alpha_b, \beta_b, \alpha_a + 1, \beta_a) \\
&=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)}
(1 - h(\alpha_a, \beta_a, \alpha_b + 1, \beta_b)) -
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)}
(1 - h(\alpha_a + 1, \beta_a, \alpha_b, \beta_b)) \\
&=
\frac{B(\alpha_b + 1, \beta_b)}{B(\alpha_b, \beta_b)} -
\frac{B(\alpha_a + 1, \beta_a)}{B(\alpha_a, \beta_a)} -
(L_1 - L_2) \\
&=
\Delta - (L_1 - L_2).
\end{align}
$$
Therefore,
$$
\begin{align}
E(U(A, B))
&= g (G_1 - G_2) + l (L_1 - L_2) \\
&= g (G_1 - G_2) + l (\Delta - (G_1 - G_2)) \\
&= (g - l) (G_1 - G_2) + l \, \Delta.
\end{align}
$$
